# LeaderWorkerSet deployment with RDMA support
# Example: helm install sglang-lws ./helm-charts/sglang -f ./helm-charts/sglang/examples/lws-rdma.yaml
# Prerequisites: LeaderWorkerSet CRD must be installed

deploymentMode: "lws"

global:
  image:
    repository: lmsysorg/sglang
    tag: latest
    pullPolicy: Always
  
  model:
    path: "/data/models/deepseek-v3-moe"
    trustRemoteCode: true
    
  resources:
    gpu: 8
    memory: "64Gi"

lws:
  enabled: true
  replicas: 1
  groupSize: 2
  
  restartPolicy: "RecreateGroupOnPodRestart"
  
  tensorParallelSize: 16
  
  server:
    host: "0.0.0.0"
    port: 40000
    memFractionStatic: "0.93"
    torchCompileMaxBs: "8"
    maxRunningRequests: "20"
    
  nccl:
    ibGidIndex: "3"
    debug: "INFO"  # Can be INFO, WARN, or TRACE for debugging
    
  service:
    type: LoadBalancer
    port: 40000
    targetPort: 40000

storage:
  shm:
    enabled: true
    size: "32Gi"
    
  model:
    type: "hostPath"
    hostPath:
      path: "/data/models/deepseek-v3-moe"
      type: "DirectoryOrCreate"
    
  huggingfaceCache:
    enabled: true
    hostPath: "/root/.cache/huggingface"

# RDMA/InfiniBand configuration
rdma:
  enabled: true
  mountInfiniBand: true
  infinibandPath: "/dev/infiniband"

# Security configuration (required for RDMA)
security:
  privileged: true

# Networking configuration (required for RDMA)
networking:
  hostNetwork: true
  hostIPC: true

# Node scheduling for multi-GPU nodes
nodeSelector:
  node-type: gpu-worker

tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

# Ensure pods are scheduled on different nodes
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: sglang
        topologyKey: kubernetes.io/hostname

serviceAccount:
  create: true
